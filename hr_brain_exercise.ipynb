{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Taste Test - Exercise on IBM HR Data\n",
    "## brAIn.hr\n",
    "\n",
    "### 3. The Data Science Workflow (General Assembly Model)\n",
    "\n",
    "![GA Pipeline](./assets/general_assembly_pipeline.png)\n",
    "\n",
    "- Frame - Problems & Hypotheses \n",
    "- Prepare - Ingestion & Cleaning\n",
    "- Analyze - Studying the Data\n",
    "- Interpret - Inference & Prediction\n",
    "- Communicate/Deploy - Enabling Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practicing Data Science & Machine Learning skills is going to require that you get hands-on with datasets solving problems that are interesting to you. Fortunately, there are now tons of places you can go to access data and practice running models. The most prominent one is Kaggle. They run paid competitions to see whose models can drive the most accurate results, and you can see other people's projects and approaches to assessing data and running predictive models. \n",
    "\n",
    "Even better than Kaggle is working with data that you're already using at work. If you're able to install python in your work environment, then you have the best source of all to practice.\n",
    "\n",
    "Today's dataset should be in the repository folder you downloaded, but if not, access it here: [Kaggle: IBM](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we dive into the data...\n",
    "\n",
    "The cell below installs a tool vital for graphing and visualizing some of our machine learning models. \n",
    "\n",
    "Notice the code comments that begin with \"#\". For your notebooks and code to be explainable, you have to use comments and docstrings \"\"\" \"\"\" heavily. You'll see more of this as you scroll through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install -c pyviz --yes --prefix {sys.prefix} hvplot\n",
    "#!conda install jupyterlab\n",
    "#!jupyter labextension install -y @pyviz/jupyterlab_pyviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below installs a tool vital for graphing and visualizing some of our machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install --prefix {sys.prefix} pydotplus  # enables visualization of decision trees\n",
    "!pip install --prefix {sys.prefix} scikit-plot  # enables visualization of machine learning metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM HR Data Project\n",
    "\n",
    "IBM released a simulated dataset tracking various factors related to IBM employment: [Kaggle: IBM](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset)\n",
    "\n",
    "Our job is to use this data to understand and predict who is likely to quit their job. \n",
    "\n",
    "Most of the features (column names) are self explanatory (DailyRate: money per day, YearsAtCompany: number of years at IBM, etc)\n",
    "\n",
    "However, the survey data columns need some explanation: \n",
    "\n",
    "- Education (1 'Below College', 2 'College', 3 'Bachelor', 4 'Master', 5 'Doctor')\n",
    "\n",
    "- EnvironmentSatisfaction (1 'Low', 2 'Medium', 3 'High', 4 'Very High')\n",
    "\n",
    "- JobInvolvement (1 'Low', 2 'Medium', 3 'High', 4 'Very High')\n",
    "\n",
    "- JobSatisfaction (1 'Low', 2 'Medium', 3 'High', 4 'Very High')\n",
    "\n",
    "- PerformanceRating (1 'Low', 2 'Good', 3 'Excellent', 4 'Outstanding')\n",
    "\n",
    "- RelationshipSatisfaction (1 'Low', 2 'Medium', 3 'High', 4 'Very High')\n",
    "\n",
    "- WorkLifeBalance (1 'Bad', 2 'Good', 3 'Better', 4 'Best')\n",
    "\n",
    "### Framing\n",
    "\n",
    "Because the ask of this project is about attrition, we'll frame our null hypothesis in exactly that way.\n",
    "\n",
    "The null hypothesis is that we cannot predict attrition better than randomly selecting or by taking the average likelihood.\n",
    "\n",
    "We reject the null if we can demonstrate statistically significant improvement over random & average likelihood as baseline.\n",
    "\n",
    "If we're able to do this, it then makes sense to explore how to optimize and automate our predictive and prescriptive power.\n",
    "\n",
    "Not an easy task, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "\n",
    "In this step, we import the that we want to work with in order to assess and visualize our data. You'll typically see this step at the top of any python project you see. Afterwards, we'll ingest and process our data.\n",
    "\n",
    "The notes here are a bit more descriptive than what you'll commonly see, but you can cut and paste the block below for a good, cookie cutter start to many of your future analytics tasks!\n",
    "\n",
    "Throughout the project, pay attention to the use of comments (#) and docstrings(\"\"\" \"\"\"). These help your code's readability. You'll forget why you added certain features or functions, and other people may look at your code and not even have a clue. Well commented and documented code solves that problem and helps your science!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # This open source marvel is your key to deep analysis and manipulation of data\n",
    "import matplotlib.pyplot as plt  # This is a low-level plotting language. Not friendly, but deep control\n",
    "import seaborn as sns  # This is a friendlier, higher level statistical plotting API based on matplotlib\n",
    "import numpy as np  # Matrix math & linear algebra\n",
    "\n",
    "from tqdm import tqdm  # This is a progress bar that comes in handy to monitor lengthy operations stress-free\n",
    "from IPython.display import display  # This gives more visibility options when running commands\n",
    "\n",
    "plt.style.use(\"seaborn-darkgrid\")  # A plotting style to make our plots look more appealing by default. \n",
    "pd.set_option(\"max_columns\", 100)  # Allows us to adjust the number of columns a DataFrame will show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hooray! You're assigning your first variable. In python, you assign \n",
    "ibm_hr_path = \"./data/WA_Fn-UseC_-HR-Employee-Attrition.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Ways You'll Interact with Variables\n",
    "\n",
    "Variables make it way easier to store, compare, and manipulate values that represent your data or real-world numbers!\n",
    "\n",
    "You assign variables simply by declaring a name, using an equal sign, and inputting a value or expression (Example):\n",
    "- variable = 20 + 7\n",
    "- print(variable)\n",
    "- output: 27\n",
    "\n",
    "Common variables you'll interact with are strings(str), integers (int), and floats (float). \n",
    "\n",
    "Strings store text, integers store numbers with no decimals, and floats store numbers with decimals.\n",
    "\n",
    "Strings consume the most memory of these types while ints consume the least. This matters heavily when working in datasets that have millions of rows, so it's important to be mindful!\n",
    "\n",
    "We'll look at how these data types can interact with each other at a high level, but there are many more ways than this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ibm_hr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_length = len(ibm_hr_path)\n",
    "print(path_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(len(ibm_hr_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_math = len(ibm_hr_path) * .59546\n",
    "print(round(path_math, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_length+path_math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(path_math))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions: What they are and why they're heavily used in this notebook\n",
    "![Algorithm](./assets/algorithm.jpg \"Image Source: https://www.verywellmind.com/what-is-an-algorithm-2794807\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions allow you to define a set of tasks and conditions so that you can drive repeatable results. You'll see them frequently in this notebook and explore the awesomeness of not having to repeat yourself. \n",
    "\n",
    "When experimenting, rather than designing and declaring the same codeblocks with multiple data sources, functions allow you to create algorithms that you can run, test, & tweak at will. \n",
    "\n",
    "It's easy to design functions and then forget what they do. Make heavy use of docstrings \"\"\" \"\"\" and code comments so that your functions are easily understood and remembered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in(path):\n",
    "    \"\"\"Reads in and initially processes the IBM dataset. Takes in the path to the file as a string\n",
    "    :path(str) - use the relative or absolute filepath to point to a data source\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(path)  # Read in the file\n",
    "    display(df.info())  # Display counts for missing data in columns and memory usage\n",
    "    print(\"\\nNumeric Columns\")\n",
    "    display(df.describe())  # Descriptive statistics for each column\n",
    "    print(\"Categorical Columns\")\n",
    "    categorical_columns = list(df.dtypes[df.dtypes == \"object\"].index)\n",
    "    display(df[categorical_columns].describe())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experimented with assigning strings, floats and integers to variables. What type of data structure are we storing in the line below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_hr_data_initial = read_in(ibm_hr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put on our stat hats. Does it make sense to keep all these columns? Are there potentially any columns that aren't conveying meaning or information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throw_away_columns = [\"EmployeeCount\", \"StandardHours\", \"Over18\"]  # This is a list of values. Lists can contain any number of other data structures:\n",
    "                                                                   # integers, strings, boolean, floats, other lists, dictionaries, tuples, etc.!\n",
    "    \n",
    "ibm_hr_data = ibm_hr_data_initial.drop(columns=throw_away_columns)  # we passed the list to the \"drop()\" function to tell it to discard these columns\n",
    "ibm_hr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ibm_hr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ibm_hr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3 Classes of Analytics](./assets/analytics_classes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we've identified people who are at-risk for attrition...\n",
    "What are some things we can and should do?\n",
    "\n",
    "A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we begin with descriptive statistics that form the bedrock for our predictions and suggestions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"value_counts()\" liberally to quickly count up values in columns. Notice how\n",
    "# convenient is to count up categorical values\n",
    "\n",
    "# Notice also the ease by which we can run calculations on these findings. \n",
    "\n",
    "attrition_value_counts = ibm_hr_data.Attrition.value_counts()\n",
    "display(attrition_value_counts, \n",
    "        str(((attrition_value_counts[1]/attrition_value_counts[0])*100).round(2)) + \"% rate of attrition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Data Cleanup\n",
    "# We move back and forth between \"Prepare\" and \"Analyze\" as we discover more flaws in the data.\n",
    "# Here, we're storing \"Yes\" & \"No\" as boolean values for more efficient math & machine learning operations.\n",
    "\n",
    "ibm_hr_data.Attrition = np.where(ibm_hr_data.Attrition == \"Yes\", True, False)  # Conditional logic to change data from string to boolean\n",
    "ibm_hr_data.OverTime = np.where(ibm_hr_data.OverTime == \"Yes\", True, False)  # Conditional logic to change data from string to boolean\n",
    "ibm_hr_data.EducationField = ibm_hr_data.EducationField.str.strip()  # the .strip() function cleans white space before and after text\n",
    "display(ibm_hr_data.Attrition.describe())  # describe() lets us quickly view relevant stats on a particuar variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little more on boolean values.\n",
    "\n",
    "These represent True(1) and False(0). That's right, True stores as \"1\" and false stores as \"0\". Below, we play with boolean values and show what they look like interacting with other numbers. \n",
    "\n",
    "They're preferrable over strings when possible because they take up less space, and you can derive details by doing simple math. For example, if you want to find out how many \"True\" values are in a True/False column, just add the values values of that column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(type(True), type(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True + 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "False + 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the numeric value of True? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_hr_data.Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sum(ibm_hr_data.Attrition))  # Adding the values of a column to get a count of the True values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = list(ibm_hr_data.dtypes[ibm_hr_data.dtypes.isin([\"int64\", \"bool\"])].index)  # Identifying the columns we can use for numeric calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_numcols = len(numeric_columns)\n",
    "print(f\"There are {number_of_numcols} numeric columns.\")  # f-strings let us put variables right into text. extremely fast and readable way to share your calculations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_plotter(columns, groupby, data=ibm_hr_data, rot=0):\n",
    "    \"\"\"Returns a series of bar plots showing average difference of each variable.\n",
    "    \n",
    "    columns(list): list of columns you'd like to plot\n",
    "    groupby(str): the name of the column you'd like to split the values\n",
    "    data(DataFrame): the DataFrame being analyzed\n",
    "    rot(int): input number of degrees to rotate the xticks for better aesthetics & readability\"\"\"\n",
    "    \n",
    "    # We want this to run quickly and effectively every time. We also want our users to understand\n",
    "    # how to use the function. The docstring above helps, but sometimes they'll get something wrong\n",
    "    # and fail to read the instructions. Below, you see that we can create our own error messages.\n",
    "    # The idea behind this one is to let the user know that they've entered the wrong data, but there\n",
    "    # are a wealth of options they can choose instead. \n",
    "    #\n",
    "    # To err is human. To raise errors is divine.\n",
    "    column_names = list(data.columns)\n",
    "    \n",
    "    # if statements allow for conditional logic, nested conditions, and powerful controls over\n",
    "    # variables and outcomes\n",
    "    if (groupby not in column_names):\n",
    "        raise ValueError(f\"Make sure that columns & groupby are in the columns of your data: \\n{column_names}\")  # F-strings make it easy and fast to explain your calculations\n",
    "    \n",
    "    # The meat of this algorithm is how it recursively generates graphs. The \"for\" loop below is \n",
    "    # telling the function to do some operation on each column of data we passed until it's\n",
    "    # processed all of the columns. Only then does it execute the next code that's not indented.\n",
    "    fontsize = 15\n",
    "    plt.figure(figsize=(18, 20))   \n",
    "    for index, col in enumerate(columns):\n",
    "        plt.subplot(12, 3, index+1)\n",
    "        sns.barplot(data=data, x=groupby, y=col)\n",
    "        plt.xlabel(groupby, fontsize=fontsize-4)\n",
    "        plt.ylabel(ylabel=col, fontsize=fontsize-5)\n",
    "        plt.xticks(fontsize=fontsize-5, rotation=rot)\n",
    "        plt.yticks(fontsize=fontsize-3)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why didn't this code work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why didn't this piece of code work?\n",
    "num_plotter(columns=numeric_columns, groupby=\"Strength\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We programmed the function to raise an error if someone tried to pass a value that isn't in the columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function in action! It's visualizing how various variables relate to Attrition. Which ones stand out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plotter(columns=numeric_columns, groupby=\"Attrition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wondered how various variables related to job satisfaction. Obvious stuff: higher job satisfaction indicates lower attrition. Even if something is \"obvious\", it's much better that it be proven. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plotter(columns=numeric_columns, groupby=\"JobSatisfaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fictional dataset lacks significant gender disparity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plotter(columns=numeric_columns, groupby=\"Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like R&D positions are relatively stable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plotter(data=ibm_hr_data, columns=[\"Attrition\"], groupby=\"Department\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of educational attainment has an impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plotter(data=ibm_hr_data, columns=[\"Attrition\"], groupby=\"EducationField\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marital status has an impact as well. This could be indicative of other factors, such as age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plotter(data=ibm_hr_data, columns=[\"Attrition\"], groupby=\"MaritalStatus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales representatives are the least stable, whereas research directors are the most stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plotter(data=ibm_hr_data, columns=[\"Attrition\"], groupby=\"JobRole\", rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the difference in attrition rate for this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plotter(data=ibm_hr_data, columns=[\"Attrition\"], groupby=\"OverTime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And among the travelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plotter(data=ibm_hr_data, columns=[\"Attrition\"], groupby=\"BusinessTravel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And divided by sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plotter(data=ibm_hr_data, columns=[\"Attrition\"], groupby=\"Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack a different way to calculate the impact of these variables. Here, you see how we can chain together multiple functions to produce desired values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_differences = ibm_hr_data.groupby(\"Attrition\").mean().pct_change().iloc[1].sort_values()\n",
    "display(percent_differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the variable to more easily see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = percent_differences.plot(kind=\"bar\", figsize=(16, 8), cmap=\"viridis\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are any of these variables related to each other? We run a simple Pearson correlation to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_correlations = ibm_hr_data[numeric_columns + ['Attrition']].corr()\n",
    "display(data_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_view(correlation_table, fontsize=20):\n",
    "    \"\"\"Intakes a correlation table and outputs a heatmap. \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(correlation_table.round(2), annot=True, cmap=\"plasma\", robust=True, square=True)\n",
    "    plt.yticks(fontsize=fontsize-8)\n",
    "    plt.xticks(fontsize=fontsize-8)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to more easily see extreme values in how these numbers relate to each other, we use a heat map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_view(correlation_table=data_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a pairplot to see distributions and how variables correlate with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_difference_columns = percent_differences.abs()[:10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(ibm_hr_data[top_difference_columns]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictive Modeling with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the right algorithm in sklearn...\n",
    "![ML Map](./assets/ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to Handle the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each class of algorithms comes with different considerations in how we work to obtain & measure predictions reliably. \n",
    "\n",
    "Because we're predicting a \"True\", \"False\" value, our work here will have to fall into the \"Classification\" category. \n",
    "\n",
    "What we're concerned with is how well the model outputs a probability that an employee will quit versus stay. \n",
    "\n",
    "Classification algorithms look for patterns in the data that are related to the output variable, the \"y\" variable, that we're trying to predict. Companies value these algorithms highly when they're able to perform well on new instances or new cases not seen in the original dataset. New cases not seen in the original data are called out-of-sample.\n",
    "\n",
    "- Why is it important to perform well on out-of-sample data? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we prepare for how well the model will perform on out-of-sample data, considering we can only train based on data we have available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_prep(df, features, target):\n",
    "    \"\"\"Input a DataFrame, a list of features, and a target column in order to get reliable\n",
    "    training and test datasets that we can use to train and measure models.\n",
    "    \n",
    "    df(DataFrame): The DataFrame we're modeling\n",
    "    features(list): Always input this as a list. These are the columns we're using to train the model. Always enter this as a list, and exclude your target variable.\n",
    "    target(str): This is the name of the target column\"\"\"\n",
    "        \n",
    "    if target in features:\n",
    "        raise ValueError(\"Your target is in your feature set. Please exclude your target variable from your features\") \n",
    "    \n",
    "    y = df[target]\n",
    "    df = df[features].copy()\n",
    "    \n",
    "    categorical_features = list(df[features].dtypes[df.dtypes == \"object\"].index)\n",
    "    \n",
    "    if len(categorical_features) > 0:\n",
    "        print(\"Processing Categorical Features: \")\n",
    "        df = categorical_prep(df=df, categorical_columns=categorical_features)\n",
    "        \n",
    "\n",
    "        df = df.drop(columns=categorical_features)\n",
    "        features = list(df.columns)\n",
    "    \n",
    "    # Split the dataframe into the feature columns & the target column    \n",
    "    X = df[features]\n",
    "\n",
    "    # This establishes a split where the model will train on 2/3rds of the data. The \"Test\" data is the remaining 33%.\n",
    "    # By splitting into these sets, we're able to mimic the effect of performance on out-of-sample data. \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=10)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_prep(df, categorical_columns):\n",
    "    \"\"\"Prepares categorical columns for machine learning tasks\"\"\"\n",
    "    \n",
    "    # loop over the columns identified as categorical\n",
    "    for col in categorical_columns:\n",
    "        dummies = pd.get_dummies(df[col])  # form a matrix of 1's and 0's representing the categories\n",
    "        print(col)\n",
    "        dummy_col_list = list(dummies.columns)\n",
    "        \n",
    "        dummy_cols = []\n",
    "        \n",
    "        for column in dummy_col_list:\n",
    "            dummy_cols.append(str(col) + \"_\" + str(column))  # Names the columns so they're more easily understood\n",
    "            \n",
    "        dummies.columns = dummy_cols\n",
    "        \n",
    "        display(dummies.head(3))\n",
    "        \n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining my Features\n",
    "\n",
    "Feature engineering is the process of selecting, creating, or processing features to prepare for machine learning algorithms.\n",
    "\n",
    "Above, we saw a numeric and categorical features that showed high association with attrition. We want to incorporate these into our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_hr_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Attrition\"\n",
    "exclude_columns = [\"Gender\", \"MaritalStatus\"]\n",
    "exclude_columns.append(target)\n",
    "features = [feature for feature in ibm_hr_data.columns if feature not in exclude_columns]  # List comprehensions are a way to quickly populate your lists\n",
    "                                                                                           # Notice that it's iterating over each column\n",
    "    \n",
    "print(features, f\"\\n\\nWe're starting with {len(features)} features, but further engineering could yield more features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Flow](./assets/data_flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data_prep(df=ibm_hr_data, features=features, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .shape is a fast, common way to see how many rows and columns a dataframe, matrix, or an array has. \n",
    "# the first number is number of rows, the second is number of columns\n",
    "\n",
    "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification algorithms. In reality, there are many others to try,\n",
    "# but given the challenge at hand, these are highly explainable, fast,\n",
    "# and robust to the categorical data we're dealing with\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest\n",
    "\n",
    "# Visualization tools that will help us visualize our decision tree\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "def tree_fit_viz(X_train, y_train, depth=4, viz=True):\n",
    "    \"\"\"Fits a decision tree to prepared training data.\n",
    "    \n",
    "    Make sure that you've appropriately divided the data before \n",
    "    fitting your model. With viz set to True, returns a \n",
    "    visualization of a single decision tree fit to the dat.\"\"\"\n",
    "    \n",
    "    # Instantiate the model\n",
    "    clf = DecisionTreeClassifier(max_depth = depth, \n",
    "                             random_state = 45)\n",
    "    \n",
    "    # fit it to the data\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Visualize the decision tree if viz is set to true\n",
    "    if viz == True:\n",
    "        dot_data = StringIO()\n",
    "        export_graphviz(clf, out_file=dot_data,filled=True, rounded=True,\n",
    "                        special_characters=True, feature_names=list(X_train.columns), max_depth=depth)\n",
    "        graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "        display(Image(graph.create_png()))\n",
    "        \n",
    "        # Extract and display feature importance\n",
    "        importance = zip(list(X_train.columns), clf.feature_importances_)\n",
    "        importance_ranks = pd.Series(dict(importance))\n",
    "        importance_ranks_significant = (importance_ranks[importance_ranks > 0]).sort_values(ascending=False) * 100\n",
    "\n",
    "        ax = importance_ranks_significant.sort_values().plot(kind=\"barh\", figsize=(10,7), title=\"Feature Importance\", cmap=\"viridis\")\n",
    "        display(ax);\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = tree_fit_viz(X_train, y_train, depth=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions, Metrics, & Interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the original framing was that it's worthwhile to pursue predictive modeling only if we find that we can outperform random performance or simply applying the majority class and assuming no one will quit. Let's check our accuracy results for random, majority class application, and our decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We'll be making {len(y_test)} predictions, which is the number of values in the test set. Let's create a DataFrame to better track these results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = pd.DataFrame({\"Real Results\": list(y_test),  # The actual results form column 1\n",
    "                             \"Random Results\": np.where(np.random.randint(0,2, size= len(y_test)) == 1, True, False),  # We use a random number generator for our random baseline\n",
    "                             \"Random Probabilities\": 0.5,  # The probabilities under random results\n",
    "                             \"False Only Column\": False,  # For our \"False\" baseline, reflecting the majority class, this will project \"False\" all down the column\n",
    "                             \"False Only Probability\": 0,  # The predicted probabilities for False only\n",
    "                             \"Tree Results\": decision_tree.predict(X_test), # This column reflects what our decision tree observed\n",
    "                             \"Tree Probabilities\": decision_tree.predict_proba(X_test)[:,1]})  # Probabilities for the Decision Tree\n",
    "\n",
    "\n",
    "result_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_only_accuracy = accuracy_score(result_frame['Real Results'], result_frame['False Only Column'])\n",
    "tree_accuracy = accuracy_score(result_frame['Real Results'], result_frame['Tree Results'])\n",
    "print(f\"Predict No Attrition Accuracy Score: {false_only_accuracy}\")\n",
    "print(f\"Decision Tree Accuracy Score: {tree_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our decision tree only barely outperformed saying everything is false! Why would we want to go through the effort of making and using a decision tree? Or should we just find another model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try another model (random forest) and use metrics that are actually effective for these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def random_forest_fit(X_train, y_train, depth, trees):\n",
    "    \"\"\"Instantiate and output a random forest model based on data and inputs\"\"\"\n",
    "    \n",
    "    rf = RandomForestClassifier(max_depth=depth, n_estimators=trees, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    return rf\n",
    "\n",
    "%time random_forest = random_forest_fit(X_train, y_train, depth=10, trees=1000)\n",
    "\n",
    "result_frame['Random Forest Results'] = random_forest.predict(X_test)\n",
    "result_frame[\"Random Forest Probabilities\"] = random_forest.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the result of running a pure accuracy metric against the numbers.\n",
    "model_results = [\"Real Results\", \"Random Results\", \"Tree Results\", \"Random Forest Results\"]\n",
    "raw_accuracy_scores = result_frame[model_results].apply(lambda x: (accuracy_score(result_frame[\"Real Results\"], x)*100))\n",
    "raw_accuracy_scores.plot(kind=\"bar\", rot=45, cmap=\"viridis\")\n",
    "display(raw_accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report = pd.DataFrame({\"Accuracy Scores\": result_frame[model_results].apply(lambda x: (accuracy_score(result_frame[\"Real Results\"], x)*100)),\n",
    "                                      \"Precision Scores\": result_frame[model_results].apply(lambda x: (precision_score(result_frame[\"Real Results\"], x)*100)),                                      \n",
    "                                      \"Recall Scores\": result_frame[model_results].apply(lambda x: (recall_score(result_frame[\"Real Results\"], x)*100)),\n",
    "                                      \"F1 Scores\": result_frame[model_results].apply(lambda x: (f1_score(result_frame[\"Real Results\"], x)*100))})\n",
    "\n",
    "classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reason to believe that the Decision Tree classifier performs the best on this data. However, this is only on predicting \"True/False\" values. What if we want to select the model that's best at scoring people at various levels of risk? \n",
    "\n",
    "For that, we'll need another classification metric called Receiver Operating Characteristic Area Under the Curve, much more commonly called AUC. What this does is test model performance at various thresholds. The output is in a familiar format: between 0 and 1, the closer to 1 being the better. This helps at understanding which model to select and put into production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = decision_tree.predict_proba(X_test)\n",
    "skplt.metrics.plot_roc(y_test, y_probas, plot_micro=False, plot_macro=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = random_forest.predict_proba(X_test)\n",
    "skplt.metrics.plot_roc(y_test, y_probas, plot_micro=False, plot_macro=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_cols = ['Real Results','Random Probabilities', 'False Only Probability', \n",
    "              'Tree Probabilities', 'Random Forest Probabilities' ]\n",
    "proba_frame = result_frame[proba_cols]\n",
    "proba_roc_auc = proba_frame.iloc[:, 1:].apply(lambda x: roc_auc_score(proba_frame['Real Results'], x))*100\n",
    "proba_roc_auc.plot(kind='bar', cmap=\"viridis\", rot=45);\n",
    "display(proba_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_calc(df, model_col_name, bins=5, real_results=\"Real Results\"):\n",
    "    \"\"\"Intakes predictive model probability output and returns the relative lift of various risk categories.\n",
    "    This is a more laymen-interpretable way to look at the results than ROC curves! Interpretability is\n",
    "    key for adoption.\"\"\"\n",
    "    \n",
    "    # pd.cut() separates the results into evenly spaced bins. These are our risk categories. Think of them as star rankings.\n",
    "    df_cut = df.groupby(pd.cut(df[model_col_name], bins=bins))[real_results].agg([\"count\", \"sum\"])\n",
    "    proba_cut = df_cut[\"sum\"]/df_cut[\"count\"]\n",
    "    \n",
    "    # We'll change the bin names into easily understood numbers\n",
    "    proba_cut.index = range(1,len(proba_cut)+1)\n",
    "    df_cut.index = range(1,len(proba_cut)+1)\n",
    "    \n",
    "    # Here, we plot the results as a bar chart\n",
    "    fontsize=20\n",
    "    \n",
    "    model = model_col_name.replace('Probabilities', 'Model')\n",
    "    \n",
    "    ax = (proba_cut*100).plot(kind=\"bar\", cmap=\"viridis\", figsize=(10,6))\n",
    "    plt.suptitle(\"Attrition Probability by Risk Score Lift Chart\", fontsize=fontsize-1, y=1.01)\n",
    "    plt.title(model)\n",
    "    plt.xlabel(\"Risk Category (Higher Number is Higher Risk)\", fontsize=fontsize-3)\n",
    "    plt.ylabel(\"Actual Attrition %\", fontsize=fontsize-3)\n",
    "    \n",
    "    # And we present a DataFrame for easier analysis\n",
    "    lift_frame = pd.concat([df_cut, proba_cut*100], axis=1)\n",
    "    lift_frame.columns = [\"Employees\", \"Employee Attrition #\", \"Attrition %\"]\n",
    "    \n",
    "    return lift_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretable Communication\n",
    "\n",
    "The graphics below frame up the results of the models in a way it should be easier to understand. This buckets the models' risk scores in a familiar \"5 star\" format. Each bar represent the probability of attrition identified in each bucket based on real numbers. For a good model, higher risk buckets should indicate higher probability of attrition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_frame = lift_calc(df=proba_frame, model_col_name=\"Tree Probabilities\")\n",
    "lift_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_frame = lift_calc(df=proba_frame, model_col_name=\"Random Forest Probabilities\")\n",
    "lift_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of our modeling, we believe that the data we're collecting on employee engagement has some definite \"signal\" in predicting attrition. \n",
    "\n",
    "Attrition is potentially highly costly, especially with high replacement costs, cultural impact, and corporate knowledge lost when high quality employees leave.\n",
    "\n",
    "Because of the nuance and sensitivity of the types of interventions we might use given attrition, we are suggesting that the ML model we use be explainable, able to provide reasons for flagging employees as high attrition risk. We will evaluate between Decision Trees and Random Forests to determine which model can be the most effective at reducing undesired attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You Made It!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data](https://media.giphy.com/media/zEU2uwmialC4U/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you made it this far, that means you're serious about learning this stuff!\n",
    "\n",
    "Here are some tracks that you can use:\n",
    "- [DataQuest](https://www.dataquest.io/) - For a self-paced, structured, online journey through initial python skills to data science tasks\n",
    "- [General Assembly](https://generalassemb.ly/education/data-science-remote-online) - For those who prefer a classroom or online class environment\n",
    "\n",
    "Who to follow?\n",
    "- [Kareem Carr](https://twitter.com/kareem_carr) - For your daily dose of data science & statistcs snark\n",
    "- [Rachel Thomas](https://twitter.com/math_rachel) - Co-founder of fast.ai, Natural Language Processing (NLP) guru, & heading up data ethics initiatives\n",
    "- [Cathy O'Neil](https://twitter.com/mathbabedotorg) - Weapons of Math Destrution Author, heavy on data ethics\n",
    "- [Chris Albon](https://twitter.com/chrisalbon) - You'll see his work a ton when you're learning data analysis in pandas, extremely helpful\n",
    "- [Kevin Markham](https://twitter.com/justmarkham) - Founder of data school, prolific poster of entry level content, great community around learning pandas!\n",
    "- [Data Science Renee](https://twitter.com/BecomingDataSci) - Great guidance on becoming a data scientist, follow her blog as well!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
